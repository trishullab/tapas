# THIS FILE IS AUTOGENERATED
# CHANGES MAY BE LOST

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, TypeVar, Any, Generic, Union, Optional
from collections.abc import Callable

from abc import ABC, abstractmethod

I = TypeVar('I')
S = TypeVar('S')

import lib.abstract_token
from lib.abstract_token_construct_autogen import abstract_token, Vocab, Grammar
from lib.python_ast_construct_autogen import *
from lib.line_format_construct_autogen import InLine, NewLine, IndentLine

from pyrsistent import s, m, pmap, v, PRecord, field
from pyrsistent.typing import PMap, PSet
from queue import Queue

class AnalysisError(Exception):
    pass

class Server(ABC, Generic[I, S]): 

    def __init__(self, in_stream : Queue[abstract_token], out_stream : Queue[I]):  

        def next(inher : I) -> abstract_token: 
            out_stream.put(inher)
            return in_stream.get()

        self.next = next


    
    # return_annotation
    def analyze_token_return_annotation(self, token : Grammar, inher : I) -> S:
        assert token.options == "return_annotation"

        if False:
            pass
        
        elif token.selection == "SomeReturnAnno":
            return self.analyze_token_return_annotation_SomeReturnAnno(inher)
            

        elif token.selection == "NoReturnAnno":
            return self.analyze_token_return_annotation_NoReturnAnno(inher)
            
        else:
            raise AnalysisError()

    def analyze_return_annotation(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_return_annotation(token, inher)

    
    # return_annotation <-- SomeReturnAnno
    def analyze_token_return_annotation_SomeReturnAnno(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_return_annotation_SomeReturnAnno_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_return_annotation_SomeReturnAnno_attributes(inher, synth_children)
    

    def traverse_return_annotation_SomeReturnAnno_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_return_annotation_SomeReturnAnno_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # return_annotation <-- NoReturnAnno
    def analyze_token_return_annotation_NoReturnAnno(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_return_annotation_NoReturnAnno_attributes(inher, synth_children)
    

    def synthesize_return_annotation_NoReturnAnno_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # except_arg
    def analyze_token_except_arg(self, token : Grammar, inher : I) -> S:
        assert token.options == "except_arg"

        if False:
            pass
        
        elif token.selection == "SomeExceptArg":
            return self.analyze_token_except_arg_SomeExceptArg(inher)
            

        elif token.selection == "SomeExceptArgName":
            return self.analyze_token_except_arg_SomeExceptArgName(inher)
            

        elif token.selection == "NoExceptArg":
            return self.analyze_token_except_arg_NoExceptArg(inher)
            
        else:
            raise AnalysisError()

    def analyze_except_arg(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_except_arg(token, inher)

    
    # except_arg <-- SomeExceptArg
    def analyze_token_except_arg_SomeExceptArg(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_except_arg_SomeExceptArg_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_except_arg_SomeExceptArg_attributes(inher, synth_children)
    

    def traverse_except_arg_SomeExceptArg_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_except_arg_SomeExceptArg_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # except_arg <-- SomeExceptArgName
    def analyze_token_except_arg_SomeExceptArgName(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_except_arg_SomeExceptArgName_content(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_str(self.traverse_except_arg_SomeExceptArgName_name(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_except_arg_SomeExceptArgName_attributes(inher, synth_children)
    

    def traverse_except_arg_SomeExceptArgName_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_except_arg_SomeExceptArgName_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_except_arg_SomeExceptArgName_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # except_arg <-- NoExceptArg
    def analyze_token_except_arg_NoExceptArg(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_except_arg_NoExceptArg_attributes(inher, synth_children)
    

    def synthesize_except_arg_NoExceptArg_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # param_annotation
    def analyze_token_param_annotation(self, token : Grammar, inher : I) -> S:
        assert token.options == "param_annotation"

        if False:
            pass
        
        elif token.selection == "SomeParamAnno":
            return self.analyze_token_param_annotation_SomeParamAnno(inher)
            

        elif token.selection == "NoParamAnno":
            return self.analyze_token_param_annotation_NoParamAnno(inher)
            
        else:
            raise AnalysisError()

    def analyze_param_annotation(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_param_annotation(token, inher)

    
    # param_annotation <-- SomeParamAnno
    def analyze_token_param_annotation_SomeParamAnno(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_param_annotation_SomeParamAnno_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_param_annotation_SomeParamAnno_attributes(inher, synth_children)
    

    def traverse_param_annotation_SomeParamAnno_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_param_annotation_SomeParamAnno_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # param_annotation <-- NoParamAnno
    def analyze_token_param_annotation_NoParamAnno(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_param_annotation_NoParamAnno_attributes(inher, synth_children)
    

    def synthesize_param_annotation_NoParamAnno_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # param_default
    def analyze_token_param_default(self, token : Grammar, inher : I) -> S:
        assert token.options == "param_default"

        if False:
            pass
        
        elif token.selection == "SomeParamDefault":
            return self.analyze_token_param_default_SomeParamDefault(inher)
            

        elif token.selection == "NoParamDefault":
            return self.analyze_token_param_default_NoParamDefault(inher)
            
        else:
            raise AnalysisError()

    def analyze_param_default(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_param_default(token, inher)

    
    # param_default <-- SomeParamDefault
    def analyze_token_param_default_SomeParamDefault(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_param_default_SomeParamDefault_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_param_default_SomeParamDefault_attributes(inher, synth_children)
    

    def traverse_param_default_SomeParamDefault_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_param_default_SomeParamDefault_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # param_default <-- NoParamDefault
    def analyze_token_param_default_NoParamDefault(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_param_default_NoParamDefault_attributes(inher, synth_children)
    

    def synthesize_param_default_NoParamDefault_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # parameters_d
    def analyze_token_parameters_d(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "parameters_d"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsKwParam": 
            return self.analyze_token_parameters_d_ConsKwParam(inher, children, stack_result, stack)
            

        elif rule_name == "SingleKwParam": 
            return self.analyze_token_parameters_d_SingleKwParam(inher, children, stack_result, stack)
            

        elif rule_name == "DictionarySplatParam": 
            return self.analyze_token_parameters_d_DictionarySplatParam(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_parameters_d(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_parameters_d(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # parameters_d <-- ConsKwParam
    def analyze_token_parameters_d_ConsKwParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_d_ConsKwParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_d_ConsKwParam_head(inher, children))
            stack.append((Grammar("parameters_d", "ConsKwParam"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("parameters_d", "ConsKwParam"), inher, children))

            # add on child node 
            child_inher = self.traverse_parameters_d_ConsKwParam_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_parameters_d_ConsKwParam_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_parameters_d_ConsKwParam_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_d_ConsKwParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_d <-- SingleKwParam
    def analyze_token_parameters_d_SingleKwParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_d_SingleKwParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_d_SingleKwParam_content(inher, children))
            stack.append((Grammar("parameters_d", "SingleKwParam"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_parameters_d_SingleKwParam_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_d_SingleKwParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_d <-- DictionarySplatParam
    def analyze_token_parameters_d_DictionarySplatParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_d_DictionarySplatParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_d_DictionarySplatParam_content(inher, children))
            stack.append((Grammar("parameters_d", "DictionarySplatParam"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_parameters_d_DictionarySplatParam_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_d_DictionarySplatParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # parameters_c
    def analyze_token_parameters_c(self, token : Grammar, inher : I) -> S:
        assert token.options == "parameters_c"

        if False:
            pass
        
        elif token.selection == "SingleListSplatParam":
            return self.analyze_token_parameters_c_SingleListSplatParam(inher)
            

        elif token.selection == "TransListSplatParam":
            return self.analyze_token_parameters_c_TransListSplatParam(inher)
            

        elif token.selection == "ParamsD":
            return self.analyze_token_parameters_c_ParamsD(inher)
            
        else:
            raise AnalysisError()

    def analyze_parameters_c(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_parameters_c(token, inher)

    
    # parameters_c <-- SingleListSplatParam
    def analyze_token_parameters_c_SingleListSplatParam(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_Param(self.traverse_parameters_c_SingleListSplatParam_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_parameters_c_SingleListSplatParam_attributes(inher, synth_children)
    

    def traverse_parameters_c_SingleListSplatParam_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_c_SingleListSplatParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_c <-- TransListSplatParam
    def analyze_token_parameters_c_TransListSplatParam(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_Param(self.traverse_parameters_c_TransListSplatParam_head(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_parameters_d(self.traverse_parameters_c_TransListSplatParam_tail(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_parameters_c_TransListSplatParam_attributes(inher, synth_children)
    

    def traverse_parameters_c_TransListSplatParam_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_parameters_c_TransListSplatParam_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_c_TransListSplatParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_c <-- ParamsD
    def analyze_token_parameters_c_ParamsD(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_parameters_d(self.traverse_parameters_c_ParamsD_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_parameters_c_ParamsD_attributes(inher, synth_children)
    

    def traverse_parameters_c_ParamsD_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_c_ParamsD_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # parameters_b
    def analyze_token_parameters_b(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "parameters_b"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsParam": 
            return self.analyze_token_parameters_b_ConsParam(inher, children, stack_result, stack)
            

        elif rule_name == "SingleParam": 
            return self.analyze_token_parameters_b_SingleParam(inher, children, stack_result, stack)
            

        elif rule_name == "ParamsC": 
            return self.analyze_token_parameters_b_ParamsC(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_parameters_b(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_parameters_b(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # parameters_b <-- ConsParam
    def analyze_token_parameters_b_ConsParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_b_ConsParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_b_ConsParam_head(inher, children))
            stack.append((Grammar("parameters_b", "ConsParam"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("parameters_b", "ConsParam"), inher, children))

            # add on child node 
            child_inher = self.traverse_parameters_b_ConsParam_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_parameters_b_ConsParam_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_parameters_b_ConsParam_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_b_ConsParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_b <-- SingleParam
    def analyze_token_parameters_b_SingleParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_b_SingleParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_b_SingleParam_content(inher, children))
            stack.append((Grammar("parameters_b", "SingleParam"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_parameters_b_SingleParam_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_b_SingleParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_b <-- ParamsC
    def analyze_token_parameters_b_ParamsC(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_b_ParamsC_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_parameters_c(self.traverse_parameters_b_ParamsC_content(inher, children))
            stack.append((Grammar("parameters_b", "ParamsC"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_parameters_b_ParamsC_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_b_ParamsC_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # parameters_a
    def analyze_token_parameters_a(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "parameters_a"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsPosParam": 
            return self.analyze_token_parameters_a_ConsPosParam(inher, children, stack_result, stack)
            

        elif rule_name == "SinglePosParam": 
            return self.analyze_token_parameters_a_SinglePosParam(inher, children, stack_result, stack)
            

        elif rule_name == "TransPosParam": 
            return self.analyze_token_parameters_a_TransPosParam(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_parameters_a(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_parameters_a(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # parameters_a <-- ConsPosParam
    def analyze_token_parameters_a_ConsPosParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_a_ConsPosParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_a_ConsPosParam_head(inher, children))
            stack.append((Grammar("parameters_a", "ConsPosParam"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("parameters_a", "ConsPosParam"), inher, children))

            # add on child node 
            child_inher = self.traverse_parameters_a_ConsPosParam_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_parameters_a_ConsPosParam_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_parameters_a_ConsPosParam_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_a_ConsPosParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_a <-- SinglePosParam
    def analyze_token_parameters_a_SinglePosParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_a_SinglePosParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_a_SinglePosParam_content(inher, children))
            stack.append((Grammar("parameters_a", "SinglePosParam"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_parameters_a_SinglePosParam_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_a_SinglePosParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters_a <-- TransPosParam
    def analyze_token_parameters_a_TransPosParam(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_parameters_a_TransPosParam_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_Param(self.traverse_parameters_a_TransPosParam_head(inher, children))
            stack.append((Grammar("parameters_a", "TransPosParam"), inher, children + tuple([child_synth])))
            return None
            

        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_parameters_b(self.traverse_parameters_a_TransPosParam_tail(inher, children))
            stack.append((Grammar("parameters_a", "TransPosParam"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_parameters_a_TransPosParam_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_parameters_a_TransPosParam_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_a_TransPosParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # parameters
    def analyze_token_parameters(self, token : Grammar, inher : I) -> S:
        assert token.options == "parameters"

        if False:
            pass
        
        elif token.selection == "ParamsA":
            return self.analyze_token_parameters_ParamsA(inher)
            

        elif token.selection == "ParamsB":
            return self.analyze_token_parameters_ParamsB(inher)
            

        elif token.selection == "NoParam":
            return self.analyze_token_parameters_NoParam(inher)
            
        else:
            raise AnalysisError()

    def analyze_parameters(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_parameters(token, inher)

    
    # parameters <-- ParamsA
    def analyze_token_parameters_ParamsA(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_parameters_a(self.traverse_parameters_ParamsA_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_parameters_ParamsA_attributes(inher, synth_children)
    

    def traverse_parameters_ParamsA_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_ParamsA_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters <-- ParamsB
    def analyze_token_parameters_ParamsB(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_parameters_b(self.traverse_parameters_ParamsB_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_parameters_ParamsB_attributes(inher, synth_children)
    

    def traverse_parameters_ParamsB_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_parameters_ParamsB_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # parameters <-- NoParam
    def analyze_token_parameters_NoParam(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_parameters_NoParam_attributes(inher, synth_children)
    

    def synthesize_parameters_NoParam_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # keyword
    def analyze_token_keyword(self, token : Grammar, inher : I) -> S:
        assert token.options == "keyword"

        if False:
            pass
        
        elif token.selection == "NamedKeyword":
            return self.analyze_token_keyword_NamedKeyword(inher)
            

        elif token.selection == "SplatKeyword":
            return self.analyze_token_keyword_SplatKeyword(inher)
            
        else:
            raise AnalysisError()

    def analyze_keyword(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_keyword(token, inher)

    
    # keyword <-- NamedKeyword
    def analyze_token_keyword_NamedKeyword(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_keyword_NamedKeyword_name(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_keyword_NamedKeyword_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_keyword_NamedKeyword_attributes(inher, synth_children)
    

    def traverse_keyword_NamedKeyword_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_keyword_NamedKeyword_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_keyword_NamedKeyword_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # keyword <-- SplatKeyword
    def analyze_token_keyword_SplatKeyword(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_keyword_SplatKeyword_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_keyword_SplatKeyword_attributes(inher, synth_children)
    

    def traverse_keyword_SplatKeyword_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_keyword_SplatKeyword_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # import_name
    def analyze_token_import_name(self, token : Grammar, inher : I) -> S:
        assert token.options == "import_name"

        if False:
            pass
        
        elif token.selection == "ImportNameAlias":
            return self.analyze_token_import_name_ImportNameAlias(inher)
            

        elif token.selection == "ImportNameOnly":
            return self.analyze_token_import_name_ImportNameOnly(inher)
            
        else:
            raise AnalysisError()

    def analyze_import_name(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_import_name(token, inher)

    
    # import_name <-- ImportNameAlias
    def analyze_token_import_name_ImportNameAlias(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_import_name_ImportNameAlias_name(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_str(self.traverse_import_name_ImportNameAlias_alias(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_import_name_ImportNameAlias_attributes(inher, synth_children)
    

    def traverse_import_name_ImportNameAlias_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_import_name_ImportNameAlias_alias(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_import_name_ImportNameAlias_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # import_name <-- ImportNameOnly
    def analyze_token_import_name_ImportNameOnly(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_import_name_ImportNameOnly_name(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_import_name_ImportNameOnly_attributes(inher, synth_children)
    

    def traverse_import_name_ImportNameOnly_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_import_name_ImportNameOnly_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # with_item
    def analyze_token_with_item(self, token : Grammar, inher : I) -> S:
        assert token.options == "with_item"

        if False:
            pass
        
        elif token.selection == "WithItemAlias":
            return self.analyze_token_with_item_WithItemAlias(inher)
            

        elif token.selection == "WithItemOnly":
            return self.analyze_token_with_item_WithItemOnly(inher)
            
        else:
            raise AnalysisError()

    def analyze_with_item(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_with_item(token, inher)

    
    # with_item <-- WithItemAlias
    def analyze_token_with_item_WithItemAlias(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_with_item_WithItemAlias_content(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_with_item_WithItemAlias_alias(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_with_item_WithItemAlias_attributes(inher, synth_children)
    

    def traverse_with_item_WithItemAlias_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_with_item_WithItemAlias_alias(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_with_item_WithItemAlias_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # with_item <-- WithItemOnly
    def analyze_token_with_item_WithItemOnly(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_with_item_WithItemOnly_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_with_item_WithItemOnly_attributes(inher, synth_children)
    

    def traverse_with_item_WithItemOnly_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_with_item_WithItemOnly_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # bases
    def analyze_token_bases(self, token : Grammar, inher : I) -> S:
        assert token.options == "bases"

        if False:
            pass
        
        elif token.selection == "SomeBases":
            return self.analyze_token_bases_SomeBases(inher)
            

        elif token.selection == "NoBases":
            return self.analyze_token_bases_NoBases(inher)
            
        else:
            raise AnalysisError()

    def analyze_bases(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_bases(token, inher)

    
    # bases <-- SomeBases
    def analyze_token_bases_SomeBases(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_bases_a(self.traverse_bases_SomeBases_bases(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_bases_SomeBases_attributes(inher, synth_children)
    

    def traverse_bases_SomeBases_bases(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_bases_SomeBases_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # bases <-- NoBases
    def analyze_token_bases_NoBases(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_bases_NoBases_attributes(inher, synth_children)
    

    def synthesize_bases_NoBases_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # bases_a
    def analyze_token_bases_a(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "bases_a"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsBase": 
            return self.analyze_token_bases_a_ConsBase(inher, children, stack_result, stack)
            

        elif rule_name == "SingleBase": 
            return self.analyze_token_bases_a_SingleBase(inher, children, stack_result, stack)
            

        elif rule_name == "KeywordsBase": 
            return self.analyze_token_bases_a_KeywordsBase(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_bases_a(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_bases_a(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # bases_a <-- ConsBase
    def analyze_token_bases_a_ConsBase(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_bases_a_ConsBase_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_bases_a_ConsBase_head(inher, children))
            stack.append((Grammar("bases_a", "ConsBase"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("bases_a", "ConsBase"), inher, children))

            # add on child node 
            child_inher = self.traverse_bases_a_ConsBase_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_bases_a_ConsBase_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_bases_a_ConsBase_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_bases_a_ConsBase_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # bases_a <-- SingleBase
    def analyze_token_bases_a_SingleBase(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_bases_a_SingleBase_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_bases_a_SingleBase_content(inher, children))
            stack.append((Grammar("bases_a", "SingleBase"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_bases_a_SingleBase_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_bases_a_SingleBase_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # bases_a <-- KeywordsBase
    def analyze_token_bases_a_KeywordsBase(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_bases_a_KeywordsBase_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_keywords(self.traverse_bases_a_KeywordsBase_kws(inher, children))
            stack.append((Grammar("bases_a", "KeywordsBase"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_bases_a_KeywordsBase_kws(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_bases_a_KeywordsBase_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # keywords
    def analyze_token_keywords(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "keywords"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsKeyword": 
            return self.analyze_token_keywords_ConsKeyword(inher, children, stack_result, stack)
            

        elif rule_name == "SingleKeyword": 
            return self.analyze_token_keywords_SingleKeyword(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_keywords(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_keywords(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # keywords <-- ConsKeyword
    def analyze_token_keywords_ConsKeyword(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_keywords_ConsKeyword_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_keyword(self.traverse_keywords_ConsKeyword_head(inher, children))
            stack.append((Grammar("keywords", "ConsKeyword"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("keywords", "ConsKeyword"), inher, children))

            # add on child node 
            child_inher = self.traverse_keywords_ConsKeyword_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_keywords_ConsKeyword_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_keywords_ConsKeyword_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_keywords_ConsKeyword_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # keywords <-- SingleKeyword
    def analyze_token_keywords_SingleKeyword(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_keywords_SingleKeyword_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_keyword(self.traverse_keywords_SingleKeyword_content(inher, children))
            stack.append((Grammar("keywords", "SingleKeyword"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_keywords_SingleKeyword_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_keywords_SingleKeyword_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # comparisons
    def analyze_token_comparisons(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "comparisons"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsCompareRight": 
            return self.analyze_token_comparisons_ConsCompareRight(inher, children, stack_result, stack)
            

        elif rule_name == "SingleCompareRight": 
            return self.analyze_token_comparisons_SingleCompareRight(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_comparisons(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_comparisons(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # comparisons <-- ConsCompareRight
    def analyze_token_comparisons_ConsCompareRight(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_comparisons_ConsCompareRight_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_CompareRight(self.traverse_comparisons_ConsCompareRight_head(inher, children))
            stack.append((Grammar("comparisons", "ConsCompareRight"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("comparisons", "ConsCompareRight"), inher, children))

            # add on child node 
            child_inher = self.traverse_comparisons_ConsCompareRight_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_comparisons_ConsCompareRight_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_comparisons_ConsCompareRight_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_comparisons_ConsCompareRight_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # comparisons <-- SingleCompareRight
    def analyze_token_comparisons_SingleCompareRight(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_comparisons_SingleCompareRight_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_CompareRight(self.traverse_comparisons_SingleCompareRight_content(inher, children))
            stack.append((Grammar("comparisons", "SingleCompareRight"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_comparisons_SingleCompareRight_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_comparisons_SingleCompareRight_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # option_expr
    def analyze_token_option_expr(self, token : Grammar, inher : I) -> S:
        assert token.options == "option_expr"

        if False:
            pass
        
        elif token.selection == "SomeExpr":
            return self.analyze_token_option_expr_SomeExpr(inher)
            

        elif token.selection == "NoExpr":
            return self.analyze_token_option_expr_NoExpr(inher)
            
        else:
            raise AnalysisError()

    def analyze_option_expr(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_option_expr(token, inher)

    
    # option_expr <-- SomeExpr
    def analyze_token_option_expr_SomeExpr(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_option_expr_SomeExpr_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_option_expr_SomeExpr_attributes(inher, synth_children)
    

    def traverse_option_expr_SomeExpr_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_option_expr_SomeExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # option_expr <-- NoExpr
    def analyze_token_option_expr_NoExpr(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_option_expr_NoExpr_attributes(inher, synth_children)
    

    def synthesize_option_expr_NoExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # comma_exprs
    def analyze_token_comma_exprs(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "comma_exprs"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsExpr": 
            return self.analyze_token_comma_exprs_ConsExpr(inher, children, stack_result, stack)
            

        elif rule_name == "SingleExpr": 
            return self.analyze_token_comma_exprs_SingleExpr(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_comma_exprs(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_comma_exprs(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # comma_exprs <-- ConsExpr
    def analyze_token_comma_exprs_ConsExpr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_comma_exprs_ConsExpr_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_comma_exprs_ConsExpr_head(inher, children))
            stack.append((Grammar("comma_exprs", "ConsExpr"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("comma_exprs", "ConsExpr"), inher, children))

            # add on child node 
            child_inher = self.traverse_comma_exprs_ConsExpr_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_comma_exprs_ConsExpr_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_comma_exprs_ConsExpr_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_comma_exprs_ConsExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # comma_exprs <-- SingleExpr
    def analyze_token_comma_exprs_SingleExpr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_comma_exprs_SingleExpr_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_comma_exprs_SingleExpr_content(inher, children))
            stack.append((Grammar("comma_exprs", "SingleExpr"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_comma_exprs_SingleExpr_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_comma_exprs_SingleExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # target_exprs
    def analyze_token_target_exprs(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "target_exprs"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsTargetExpr": 
            return self.analyze_token_target_exprs_ConsTargetExpr(inher, children, stack_result, stack)
            

        elif rule_name == "SingleTargetExpr": 
            return self.analyze_token_target_exprs_SingleTargetExpr(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_target_exprs(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_target_exprs(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # target_exprs <-- ConsTargetExpr
    def analyze_token_target_exprs_ConsTargetExpr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_target_exprs_ConsTargetExpr_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_target_exprs_ConsTargetExpr_head(inher, children))
            stack.append((Grammar("target_exprs", "ConsTargetExpr"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("target_exprs", "ConsTargetExpr"), inher, children))

            # add on child node 
            child_inher = self.traverse_target_exprs_ConsTargetExpr_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_target_exprs_ConsTargetExpr_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_target_exprs_ConsTargetExpr_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_target_exprs_ConsTargetExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # target_exprs <-- SingleTargetExpr
    def analyze_token_target_exprs_SingleTargetExpr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_target_exprs_SingleTargetExpr_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_target_exprs_SingleTargetExpr_content(inher, children))
            stack.append((Grammar("target_exprs", "SingleTargetExpr"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_target_exprs_SingleTargetExpr_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_target_exprs_SingleTargetExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # decorators
    def analyze_token_decorators(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "decorators"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsDec": 
            return self.analyze_token_decorators_ConsDec(inher, children, stack_result, stack)
            

        elif rule_name == "NoDec": 
            return self.analyze_token_decorators_NoDec(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_decorators(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_decorators(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # decorators <-- ConsDec
    def analyze_token_decorators_ConsDec(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_decorators_ConsDec_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_decorators_ConsDec_head(inher, children))
            stack.append((Grammar("decorators", "ConsDec"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("decorators", "ConsDec"), inher, children))

            # add on child node 
            child_inher = self.traverse_decorators_ConsDec_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_decorators_ConsDec_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_decorators_ConsDec_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_decorators_ConsDec_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # decorators <-- NoDec
    def analyze_token_decorators_NoDec(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_decorators_NoDec_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_decorators_NoDec_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # constraint_filters
    def analyze_token_constraint_filters(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "constraint_filters"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsFilter": 
            return self.analyze_token_constraint_filters_ConsFilter(inher, children, stack_result, stack)
            

        elif rule_name == "SingleFilter": 
            return self.analyze_token_constraint_filters_SingleFilter(inher, children, stack_result, stack)
            

        elif rule_name == "NoFilter": 
            return self.analyze_token_constraint_filters_NoFilter(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_constraint_filters(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_constraint_filters(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # constraint_filters <-- ConsFilter
    def analyze_token_constraint_filters_ConsFilter(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_constraint_filters_ConsFilter_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_constraint_filters_ConsFilter_head(inher, children))
            stack.append((Grammar("constraint_filters", "ConsFilter"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("constraint_filters", "ConsFilter"), inher, children))

            # add on child node 
            child_inher = self.traverse_constraint_filters_ConsFilter_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_constraint_filters_ConsFilter_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_constraint_filters_ConsFilter_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_constraint_filters_ConsFilter_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # constraint_filters <-- SingleFilter
    def analyze_token_constraint_filters_SingleFilter(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_constraint_filters_SingleFilter_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_constraint_filters_SingleFilter_content(inher, children))
            stack.append((Grammar("constraint_filters", "SingleFilter"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_constraint_filters_SingleFilter_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_constraint_filters_SingleFilter_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # constraint_filters <-- NoFilter
    def analyze_token_constraint_filters_NoFilter(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_constraint_filters_NoFilter_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_constraint_filters_NoFilter_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # sequence_string
    def analyze_token_sequence_string(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "sequence_string"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsStr": 
            return self.analyze_token_sequence_string_ConsStr(inher, children, stack_result, stack)
            

        elif rule_name == "SingleStr": 
            return self.analyze_token_sequence_string_SingleStr(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_sequence_string(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_sequence_string(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # sequence_string <-- ConsStr
    def analyze_token_sequence_string_ConsStr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_string_ConsStr_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_sequence_string_ConsStr_head(inher, children))
            stack.append((Grammar("sequence_string", "ConsStr"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("sequence_string", "ConsStr"), inher, children))

            # add on child node 
            child_inher = self.traverse_sequence_string_ConsStr_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_sequence_string_ConsStr_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_sequence_string_ConsStr_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_string_ConsStr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # sequence_string <-- SingleStr
    def analyze_token_sequence_string_SingleStr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_string_SingleStr_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_sequence_string_SingleStr_content(inher, children))
            stack.append((Grammar("sequence_string", "SingleStr"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_sequence_string_SingleStr_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_string_SingleStr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # arguments
    def analyze_token_arguments(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "arguments"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsArg": 
            return self.analyze_token_arguments_ConsArg(inher, children, stack_result, stack)
            

        elif rule_name == "SingleArg": 
            return self.analyze_token_arguments_SingleArg(inher, children, stack_result, stack)
            

        elif rule_name == "KeywordsArg": 
            return self.analyze_token_arguments_KeywordsArg(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_arguments(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_arguments(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # arguments <-- ConsArg
    def analyze_token_arguments_ConsArg(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_arguments_ConsArg_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_arguments_ConsArg_head(inher, children))
            stack.append((Grammar("arguments", "ConsArg"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("arguments", "ConsArg"), inher, children))

            # add on child node 
            child_inher = self.traverse_arguments_ConsArg_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_arguments_ConsArg_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_arguments_ConsArg_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_arguments_ConsArg_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # arguments <-- SingleArg
    def analyze_token_arguments_SingleArg(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_arguments_SingleArg_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_expr(self.traverse_arguments_SingleArg_content(inher, children))
            stack.append((Grammar("arguments", "SingleArg"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_arguments_SingleArg_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_arguments_SingleArg_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # arguments <-- KeywordsArg
    def analyze_token_arguments_KeywordsArg(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_arguments_KeywordsArg_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_keywords(self.traverse_arguments_KeywordsArg_kws(inher, children))
            stack.append((Grammar("arguments", "KeywordsArg"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_arguments_KeywordsArg_kws(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_arguments_KeywordsArg_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # dictionary_item
    def analyze_token_dictionary_item(self, token : Grammar, inher : I) -> S:
        assert token.options == "dictionary_item"

        if False:
            pass
        
        elif token.selection == "Field":
            return self.analyze_token_dictionary_item_Field(inher)
            

        elif token.selection == "DictionarySplatFields":
            return self.analyze_token_dictionary_item_DictionarySplatFields(inher)
            
        else:
            raise AnalysisError()

    def analyze_dictionary_item(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_dictionary_item(token, inher)

    
    # dictionary_item <-- Field
    def analyze_token_dictionary_item_Field(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_dictionary_item_Field_key(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_dictionary_item_Field_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_dictionary_item_Field_attributes(inher, synth_children)
    

    def traverse_dictionary_item_Field_key(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_dictionary_item_Field_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_dictionary_item_Field_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # dictionary_item <-- DictionarySplatFields
    def analyze_token_dictionary_item_DictionarySplatFields(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_dictionary_item_DictionarySplatFields_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_dictionary_item_DictionarySplatFields_attributes(inher, synth_children)
    

    def traverse_dictionary_item_DictionarySplatFields_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_dictionary_item_DictionarySplatFields_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # dictionary_content
    def analyze_token_dictionary_content(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "dictionary_content"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsDictionaryItem": 
            return self.analyze_token_dictionary_content_ConsDictionaryItem(inher, children, stack_result, stack)
            

        elif rule_name == "SingleDictionaryItem": 
            return self.analyze_token_dictionary_content_SingleDictionaryItem(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_dictionary_content(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_dictionary_content(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # dictionary_content <-- ConsDictionaryItem
    def analyze_token_dictionary_content_ConsDictionaryItem(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_dictionary_content_ConsDictionaryItem_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_dictionary_item(self.traverse_dictionary_content_ConsDictionaryItem_head(inher, children))
            stack.append((Grammar("dictionary_content", "ConsDictionaryItem"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("dictionary_content", "ConsDictionaryItem"), inher, children))

            # add on child node 
            child_inher = self.traverse_dictionary_content_ConsDictionaryItem_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_dictionary_content_ConsDictionaryItem_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_dictionary_content_ConsDictionaryItem_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_dictionary_content_ConsDictionaryItem_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # dictionary_content <-- SingleDictionaryItem
    def analyze_token_dictionary_content_SingleDictionaryItem(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_dictionary_content_SingleDictionaryItem_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_dictionary_item(self.traverse_dictionary_content_SingleDictionaryItem_content(inher, children))
            stack.append((Grammar("dictionary_content", "SingleDictionaryItem"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_dictionary_content_SingleDictionaryItem_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_dictionary_content_SingleDictionaryItem_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # sequence_name
    def analyze_token_sequence_name(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "sequence_name"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsId": 
            return self.analyze_token_sequence_name_ConsId(inher, children, stack_result, stack)
            

        elif rule_name == "SingleId": 
            return self.analyze_token_sequence_name_SingleId(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_sequence_name(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_sequence_name(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # sequence_name <-- ConsId
    def analyze_token_sequence_name_ConsId(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_name_ConsId_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_sequence_name_ConsId_head(inher, children))
            stack.append((Grammar("sequence_name", "ConsId"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("sequence_name", "ConsId"), inher, children))

            # add on child node 
            child_inher = self.traverse_sequence_name_ConsId_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_sequence_name_ConsId_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_sequence_name_ConsId_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_name_ConsId_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # sequence_name <-- SingleId
    def analyze_token_sequence_name_SingleId(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_name_SingleId_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_sequence_name_SingleId_content(inher, children))
            stack.append((Grammar("sequence_name", "SingleId"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_sequence_name_SingleId_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_name_SingleId_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # sequence_import_name
    def analyze_token_sequence_import_name(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "sequence_import_name"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsImportName": 
            return self.analyze_token_sequence_import_name_ConsImportName(inher, children, stack_result, stack)
            

        elif rule_name == "SingleImportName": 
            return self.analyze_token_sequence_import_name_SingleImportName(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_sequence_import_name(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_sequence_import_name(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # sequence_import_name <-- ConsImportName
    def analyze_token_sequence_import_name_ConsImportName(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_import_name_ConsImportName_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_import_name(self.traverse_sequence_import_name_ConsImportName_head(inher, children))
            stack.append((Grammar("sequence_import_name", "ConsImportName"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("sequence_import_name", "ConsImportName"), inher, children))

            # add on child node 
            child_inher = self.traverse_sequence_import_name_ConsImportName_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_sequence_import_name_ConsImportName_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_sequence_import_name_ConsImportName_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_import_name_ConsImportName_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # sequence_import_name <-- SingleImportName
    def analyze_token_sequence_import_name_SingleImportName(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_import_name_SingleImportName_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_import_name(self.traverse_sequence_import_name_SingleImportName_content(inher, children))
            stack.append((Grammar("sequence_import_name", "SingleImportName"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_sequence_import_name_SingleImportName_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_import_name_SingleImportName_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # sequence_with_item
    def analyze_token_sequence_with_item(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "sequence_with_item"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsWithItem": 
            return self.analyze_token_sequence_with_item_ConsWithItem(inher, children, stack_result, stack)
            

        elif rule_name == "SingleWithItem": 
            return self.analyze_token_sequence_with_item_SingleWithItem(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_sequence_with_item(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_sequence_with_item(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # sequence_with_item <-- ConsWithItem
    def analyze_token_sequence_with_item_ConsWithItem(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_with_item_ConsWithItem_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_with_item(self.traverse_sequence_with_item_ConsWithItem_head(inher, children))
            stack.append((Grammar("sequence_with_item", "ConsWithItem"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("sequence_with_item", "ConsWithItem"), inher, children))

            # add on child node 
            child_inher = self.traverse_sequence_with_item_ConsWithItem_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_sequence_with_item_ConsWithItem_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_sequence_with_item_ConsWithItem_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_with_item_ConsWithItem_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # sequence_with_item <-- SingleWithItem
    def analyze_token_sequence_with_item_SingleWithItem(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_with_item_SingleWithItem_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_with_item(self.traverse_sequence_with_item_SingleWithItem_content(inher, children))
            stack.append((Grammar("sequence_with_item", "SingleWithItem"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_sequence_with_item_SingleWithItem_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_with_item_SingleWithItem_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # module
    def analyze_token_module(self, token : Grammar, inher : I) -> S:
        assert token.options == "module"

        if False:
            pass
        
        elif token.selection == "FutureMod":
            return self.analyze_token_module_FutureMod(inher)
            

        elif token.selection == "SimpleMod":
            return self.analyze_token_module_SimpleMod(inher)
            
        else:
            raise AnalysisError()

    def analyze_module(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_module(token, inher)

    
    # module <-- FutureMod
    def analyze_token_module_FutureMod(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_sequence_import_name(self.traverse_module_FutureMod_names(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_module_FutureMod_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_module_FutureMod_attributes(inher, synth_children)
    

    def traverse_module_FutureMod_names(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_module_FutureMod_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_module_FutureMod_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # module <-- SimpleMod
    def analyze_token_module_SimpleMod(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_statements(self.traverse_module_SimpleMod_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_module_SimpleMod_attributes(inher, synth_children)
    

    def traverse_module_SimpleMod_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_module_SimpleMod_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # statements
    def analyze_token_statements(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "statements"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsStmt": 
            return self.analyze_token_statements_ConsStmt(inher, children, stack_result, stack)
            

        elif rule_name == "SingleStmt": 
            return self.analyze_token_statements_SingleStmt(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_statements(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_statements(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # statements <-- ConsStmt
    def analyze_token_statements_ConsStmt(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_statements_ConsStmt_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_stmt(self.traverse_statements_ConsStmt_head(inher, children))
            stack.append((Grammar("statements", "ConsStmt"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("statements", "ConsStmt"), inher, children))

            # add on child node 
            child_inher = self.traverse_statements_ConsStmt_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_statements_ConsStmt_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_statements_ConsStmt_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_statements_ConsStmt_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # statements <-- SingleStmt
    def analyze_token_statements_SingleStmt(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_statements_SingleStmt_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_stmt(self.traverse_statements_SingleStmt_content(inher, children))
            stack.append((Grammar("statements", "SingleStmt"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_statements_SingleStmt_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_statements_SingleStmt_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # comprehension_constraints
    def analyze_token_comprehension_constraints(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "comprehension_constraints"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsConstraint": 
            return self.analyze_token_comprehension_constraints_ConsConstraint(inher, children, stack_result, stack)
            

        elif rule_name == "SingleConstraint": 
            return self.analyze_token_comprehension_constraints_SingleConstraint(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_comprehension_constraints(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_comprehension_constraints(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # comprehension_constraints <-- ConsConstraint
    def analyze_token_comprehension_constraints_ConsConstraint(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_comprehension_constraints_ConsConstraint_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_constraint(self.traverse_comprehension_constraints_ConsConstraint_head(inher, children))
            stack.append((Grammar("comprehension_constraints", "ConsConstraint"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("comprehension_constraints", "ConsConstraint"), inher, children))

            # add on child node 
            child_inher = self.traverse_comprehension_constraints_ConsConstraint_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_comprehension_constraints_ConsConstraint_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_comprehension_constraints_ConsConstraint_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_comprehension_constraints_ConsConstraint_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # comprehension_constraints <-- SingleConstraint
    def analyze_token_comprehension_constraints_SingleConstraint(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_comprehension_constraints_SingleConstraint_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_constraint(self.traverse_comprehension_constraints_SingleConstraint_content(inher, children))
            stack.append((Grammar("comprehension_constraints", "SingleConstraint"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_comprehension_constraints_SingleConstraint_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_comprehension_constraints_SingleConstraint_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # sequence_ExceptHandler
    def analyze_token_sequence_ExceptHandler(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "sequence_ExceptHandler"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ConsExceptHandler": 
            return self.analyze_token_sequence_ExceptHandler_ConsExceptHandler(inher, children, stack_result, stack)
            

        elif rule_name == "SingleExceptHandler": 
            return self.analyze_token_sequence_ExceptHandler_SingleExceptHandler(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_sequence_ExceptHandler(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_sequence_ExceptHandler(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # sequence_ExceptHandler <-- ConsExceptHandler
    def analyze_token_sequence_ExceptHandler_ConsExceptHandler(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_ExceptHandler_ConsExceptHandler_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_ExceptHandler(self.traverse_sequence_ExceptHandler_ConsExceptHandler_head(inher, children))
            stack.append((Grammar("sequence_ExceptHandler", "ConsExceptHandler"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("sequence_ExceptHandler", "ConsExceptHandler"), inher, children))

            # add on child node 
            child_inher = self.traverse_sequence_ExceptHandler_ConsExceptHandler_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_sequence_ExceptHandler_ConsExceptHandler_head(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_sequence_ExceptHandler_ConsExceptHandler_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_ExceptHandler_ConsExceptHandler_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # sequence_ExceptHandler <-- SingleExceptHandler
    def analyze_token_sequence_ExceptHandler_SingleExceptHandler(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_sequence_ExceptHandler_SingleExceptHandler_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_ExceptHandler(self.traverse_sequence_ExceptHandler_SingleExceptHandler_content(inher, children))
            stack.append((Grammar("sequence_ExceptHandler", "SingleExceptHandler"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_sequence_ExceptHandler_SingleExceptHandler_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_sequence_ExceptHandler_SingleExceptHandler_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # conditions
    def analyze_token_conditions(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "conditions"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "ElifCond": 
            return self.analyze_token_conditions_ElifCond(inher, children, stack_result, stack)
            

        elif rule_name == "ElseCond": 
            return self.analyze_token_conditions_ElseCond(inher, children, stack_result, stack)
            

        elif rule_name == "NoCond": 
            return self.analyze_token_conditions_NoCond(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_conditions(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_conditions(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # conditions <-- ElifCond
    def analyze_token_conditions_ElifCond(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_conditions_ElifCond_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_ElifBlock(self.traverse_conditions_ElifCond_content(inher, children))
            stack.append((Grammar("conditions", "ElifCond"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("conditions", "ElifCond"), inher, children))

            # add on child node 
            child_inher = self.traverse_conditions_ElifCond_tail(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_conditions_ElifCond_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_conditions_ElifCond_tail(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_conditions_ElifCond_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # conditions <-- ElseCond
    def analyze_token_conditions_ElseCond(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_conditions_ElseCond_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_ElseBlock(self.traverse_conditions_ElseCond_content(inher, children))
            stack.append((Grammar("conditions", "ElseCond"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_conditions_ElseCond_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_conditions_ElseCond_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # conditions <-- NoCond
    def analyze_token_conditions_NoCond(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_conditions_NoCond_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_conditions_NoCond_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # function_def
    def analyze_token_function_def(self, token : Grammar, inher : I) -> S:
        assert token.options == "function_def"

        if False:
            pass
        
        elif token.selection == "FunctionDef":
            return self.analyze_token_function_def_FunctionDef(inher)
            

        elif token.selection == "AsyncFunctionDef":
            return self.analyze_token_function_def_AsyncFunctionDef(inher)
            
        else:
            raise AnalysisError()

    def analyze_function_def(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_function_def(token, inher)

    
    # function_def <-- FunctionDef
    def analyze_token_function_def_FunctionDef(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_function_def_FunctionDef_name(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_parameters(self.traverse_function_def_FunctionDef_params(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_return_annotation(self.traverse_function_def_FunctionDef_ret_anno(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_function_def_FunctionDef_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_function_def_FunctionDef_attributes(inher, synth_children)
    

    def traverse_function_def_FunctionDef_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_function_def_FunctionDef_params(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_function_def_FunctionDef_ret_anno(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_function_def_FunctionDef_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_function_def_FunctionDef_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # function_def <-- AsyncFunctionDef
    def analyze_token_function_def_AsyncFunctionDef(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_function_def_AsyncFunctionDef_name(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_parameters(self.traverse_function_def_AsyncFunctionDef_params(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_return_annotation(self.traverse_function_def_AsyncFunctionDef_ret_anno(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_function_def_AsyncFunctionDef_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_function_def_AsyncFunctionDef_attributes(inher, synth_children)
    

    def traverse_function_def_AsyncFunctionDef_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_function_def_AsyncFunctionDef_params(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_function_def_AsyncFunctionDef_ret_anno(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_function_def_AsyncFunctionDef_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_function_def_AsyncFunctionDef_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # stmt
    def analyze_token_stmt(self, token : Grammar, inher : I) -> S:
        assert token.options == "stmt"

        if False:
            pass
        
        elif token.selection == "DecFunctionDef":
            return self.analyze_token_stmt_DecFunctionDef(inher)
            

        elif token.selection == "DecClassDef":
            return self.analyze_token_stmt_DecClassDef(inher)
            

        elif token.selection == "ReturnSomething":
            return self.analyze_token_stmt_ReturnSomething(inher)
            

        elif token.selection == "Return":
            return self.analyze_token_stmt_Return(inher)
            

        elif token.selection == "Delete":
            return self.analyze_token_stmt_Delete(inher)
            

        elif token.selection == "Assign":
            return self.analyze_token_stmt_Assign(inher)
            

        elif token.selection == "AugAssign":
            return self.analyze_token_stmt_AugAssign(inher)
            

        elif token.selection == "AnnoAssign":
            return self.analyze_token_stmt_AnnoAssign(inher)
            

        elif token.selection == "AnnoDeclar":
            return self.analyze_token_stmt_AnnoDeclar(inher)
            

        elif token.selection == "For":
            return self.analyze_token_stmt_For(inher)
            

        elif token.selection == "ForElse":
            return self.analyze_token_stmt_ForElse(inher)
            

        elif token.selection == "AsyncFor":
            return self.analyze_token_stmt_AsyncFor(inher)
            

        elif token.selection == "AsyncForElse":
            return self.analyze_token_stmt_AsyncForElse(inher)
            

        elif token.selection == "While":
            return self.analyze_token_stmt_While(inher)
            

        elif token.selection == "WhileElse":
            return self.analyze_token_stmt_WhileElse(inher)
            

        elif token.selection == "If":
            return self.analyze_token_stmt_If(inher)
            

        elif token.selection == "With":
            return self.analyze_token_stmt_With(inher)
            

        elif token.selection == "AsyncWith":
            return self.analyze_token_stmt_AsyncWith(inher)
            

        elif token.selection == "Raise":
            return self.analyze_token_stmt_Raise(inher)
            

        elif token.selection == "RaiseExc":
            return self.analyze_token_stmt_RaiseExc(inher)
            

        elif token.selection == "RaiseFrom":
            return self.analyze_token_stmt_RaiseFrom(inher)
            

        elif token.selection == "Try":
            return self.analyze_token_stmt_Try(inher)
            

        elif token.selection == "TryElse":
            return self.analyze_token_stmt_TryElse(inher)
            

        elif token.selection == "TryExceptFin":
            return self.analyze_token_stmt_TryExceptFin(inher)
            

        elif token.selection == "TryFin":
            return self.analyze_token_stmt_TryFin(inher)
            

        elif token.selection == "TryElseFin":
            return self.analyze_token_stmt_TryElseFin(inher)
            

        elif token.selection == "Assert":
            return self.analyze_token_stmt_Assert(inher)
            

        elif token.selection == "AssertMsg":
            return self.analyze_token_stmt_AssertMsg(inher)
            

        elif token.selection == "Import":
            return self.analyze_token_stmt_Import(inher)
            

        elif token.selection == "ImportFrom":
            return self.analyze_token_stmt_ImportFrom(inher)
            

        elif token.selection == "ImportWildCard":
            return self.analyze_token_stmt_ImportWildCard(inher)
            

        elif token.selection == "Global":
            return self.analyze_token_stmt_Global(inher)
            

        elif token.selection == "Nonlocal":
            return self.analyze_token_stmt_Nonlocal(inher)
            

        elif token.selection == "Expr":
            return self.analyze_token_stmt_Expr(inher)
            

        elif token.selection == "Pass":
            return self.analyze_token_stmt_Pass(inher)
            

        elif token.selection == "Break":
            return self.analyze_token_stmt_Break(inher)
            

        elif token.selection == "Continue":
            return self.analyze_token_stmt_Continue(inher)
            
        else:
            raise AnalysisError()

    def analyze_stmt(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_stmt(token, inher)

    
    # stmt <-- DecFunctionDef
    def analyze_token_stmt_DecFunctionDef(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_decorators(self.traverse_stmt_DecFunctionDef_decs(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_function_def(self.traverse_stmt_DecFunctionDef_fun_def(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_DecFunctionDef_attributes(inher, synth_children)
    

    def traverse_stmt_DecFunctionDef_decs(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_DecFunctionDef_fun_def(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_DecFunctionDef_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- DecClassDef
    def analyze_token_stmt_DecClassDef(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_decorators(self.traverse_stmt_DecClassDef_decs(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_ClassDef(self.traverse_stmt_DecClassDef_class_def(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_DecClassDef_attributes(inher, synth_children)
    

    def traverse_stmt_DecClassDef_decs(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_DecClassDef_class_def(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_DecClassDef_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- ReturnSomething
    def analyze_token_stmt_ReturnSomething(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_ReturnSomething_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_ReturnSomething_attributes(inher, synth_children)
    

    def traverse_stmt_ReturnSomething_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_ReturnSomething_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Return
    def analyze_token_stmt_Return(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_stmt_Return_attributes(inher, synth_children)
    

    def synthesize_stmt_Return_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Delete
    def analyze_token_stmt_Delete(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_comma_exprs(self.traverse_stmt_Delete_targets(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Delete_attributes(inher, synth_children)
    

    def traverse_stmt_Delete_targets(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Delete_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Assign
    def analyze_token_stmt_Assign(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_target_exprs(self.traverse_stmt_Assign_targets(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_Assign_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Assign_attributes(inher, synth_children)
    

    def traverse_stmt_Assign_targets(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_Assign_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Assign_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AugAssign
    def analyze_token_stmt_AugAssign(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_AugAssign_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_operator(self.traverse_stmt_AugAssign_op(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AugAssign_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AugAssign_attributes(inher, synth_children)
    

    def traverse_stmt_AugAssign_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AugAssign_op(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AugAssign_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AugAssign_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AnnoAssign
    def analyze_token_stmt_AnnoAssign(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_AnnoAssign_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AnnoAssign_anno(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AnnoAssign_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AnnoAssign_attributes(inher, synth_children)
    

    def traverse_stmt_AnnoAssign_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AnnoAssign_anno(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AnnoAssign_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AnnoAssign_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AnnoDeclar
    def analyze_token_stmt_AnnoDeclar(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_AnnoDeclar_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AnnoDeclar_anno(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AnnoDeclar_attributes(inher, synth_children)
    

    def traverse_stmt_AnnoDeclar_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AnnoDeclar_anno(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AnnoDeclar_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- For
    def analyze_token_stmt_For(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_For_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_For_iter(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_For_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_For_attributes(inher, synth_children)
    

    def traverse_stmt_For_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_For_iter(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_For_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_For_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- ForElse
    def analyze_token_stmt_ForElse(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_ForElse_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_ForElse_iter(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_ForElse_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_ElseBlock(self.traverse_stmt_ForElse_orelse(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_ForElse_attributes(inher, synth_children)
    

    def traverse_stmt_ForElse_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_ForElse_iter(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_ForElse_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_ForElse_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_ForElse_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AsyncFor
    def analyze_token_stmt_AsyncFor(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_AsyncFor_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AsyncFor_iter(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_AsyncFor_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AsyncFor_attributes(inher, synth_children)
    

    def traverse_stmt_AsyncFor_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AsyncFor_iter(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AsyncFor_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AsyncFor_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AsyncForElse
    def analyze_token_stmt_AsyncForElse(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_AsyncForElse_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AsyncForElse_iter(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_AsyncForElse_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_ElseBlock(self.traverse_stmt_AsyncForElse_orelse(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AsyncForElse_attributes(inher, synth_children)
    

    def traverse_stmt_AsyncForElse_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AsyncForElse_iter(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AsyncForElse_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AsyncForElse_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AsyncForElse_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- While
    def analyze_token_stmt_While(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_While_test(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_While_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_While_attributes(inher, synth_children)
    

    def traverse_stmt_While_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_While_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_While_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- WhileElse
    def analyze_token_stmt_WhileElse(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_WhileElse_test(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_WhileElse_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_ElseBlock(self.traverse_stmt_WhileElse_orelse(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_WhileElse_attributes(inher, synth_children)
    

    def traverse_stmt_WhileElse_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_WhileElse_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_WhileElse_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_WhileElse_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- If
    def analyze_token_stmt_If(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_If_test(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_If_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_conditions(self.traverse_stmt_If_orelse(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_If_attributes(inher, synth_children)
    

    def traverse_stmt_If_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_If_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_If_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_If_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- With
    def analyze_token_stmt_With(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_sequence_with_item(self.traverse_stmt_With_items(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_With_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_With_attributes(inher, synth_children)
    

    def traverse_stmt_With_items(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_With_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_With_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AsyncWith
    def analyze_token_stmt_AsyncWith(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_sequence_with_item(self.traverse_stmt_AsyncWith_items(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_statements(self.traverse_stmt_AsyncWith_body(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AsyncWith_attributes(inher, synth_children)
    

    def traverse_stmt_AsyncWith_items(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AsyncWith_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AsyncWith_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Raise
    def analyze_token_stmt_Raise(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_stmt_Raise_attributes(inher, synth_children)
    

    def synthesize_stmt_Raise_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- RaiseExc
    def analyze_token_stmt_RaiseExc(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_RaiseExc_exc(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_RaiseExc_attributes(inher, synth_children)
    

    def traverse_stmt_RaiseExc_exc(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_RaiseExc_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- RaiseFrom
    def analyze_token_stmt_RaiseFrom(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_RaiseFrom_exc(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_RaiseFrom_caus(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_RaiseFrom_attributes(inher, synth_children)
    

    def traverse_stmt_RaiseFrom_exc(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_RaiseFrom_caus(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_RaiseFrom_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Try
    def analyze_token_stmt_Try(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_statements(self.traverse_stmt_Try_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_sequence_ExceptHandler(self.traverse_stmt_Try_handlers(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Try_attributes(inher, synth_children)
    

    def traverse_stmt_Try_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_Try_handlers(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Try_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- TryElse
    def analyze_token_stmt_TryElse(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_statements(self.traverse_stmt_TryElse_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_sequence_ExceptHandler(self.traverse_stmt_TryElse_handlers(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_ElseBlock(self.traverse_stmt_TryElse_orelse(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_TryElse_attributes(inher, synth_children)
    

    def traverse_stmt_TryElse_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryElse_handlers(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryElse_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_TryElse_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- TryExceptFin
    def analyze_token_stmt_TryExceptFin(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_statements(self.traverse_stmt_TryExceptFin_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_sequence_ExceptHandler(self.traverse_stmt_TryExceptFin_handlers(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_FinallyBlock(self.traverse_stmt_TryExceptFin_fin(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_TryExceptFin_attributes(inher, synth_children)
    

    def traverse_stmt_TryExceptFin_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryExceptFin_handlers(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryExceptFin_fin(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_TryExceptFin_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- TryFin
    def analyze_token_stmt_TryFin(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_statements(self.traverse_stmt_TryFin_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_FinallyBlock(self.traverse_stmt_TryFin_fin(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_TryFin_attributes(inher, synth_children)
    

    def traverse_stmt_TryFin_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryFin_fin(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_TryFin_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- TryElseFin
    def analyze_token_stmt_TryElseFin(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_statements(self.traverse_stmt_TryElseFin_body(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_sequence_ExceptHandler(self.traverse_stmt_TryElseFin_handlers(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_ElseBlock(self.traverse_stmt_TryElseFin_orelse(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_FinallyBlock(self.traverse_stmt_TryElseFin_fin(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_TryElseFin_attributes(inher, synth_children)
    

    def traverse_stmt_TryElseFin_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryElseFin_handlers(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryElseFin_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_TryElseFin_fin(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_TryElseFin_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Assert
    def analyze_token_stmt_Assert(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_Assert_test(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Assert_attributes(inher, synth_children)
    

    def traverse_stmt_Assert_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Assert_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- AssertMsg
    def analyze_token_stmt_AssertMsg(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_AssertMsg_test(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_stmt_AssertMsg_msg(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_AssertMsg_attributes(inher, synth_children)
    

    def traverse_stmt_AssertMsg_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_AssertMsg_msg(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_AssertMsg_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Import
    def analyze_token_stmt_Import(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_sequence_import_name(self.traverse_stmt_Import_names(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Import_attributes(inher, synth_children)
    

    def traverse_stmt_Import_names(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Import_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- ImportFrom
    def analyze_token_stmt_ImportFrom(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_stmt_ImportFrom_module(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_sequence_import_name(self.traverse_stmt_ImportFrom_names(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_ImportFrom_attributes(inher, synth_children)
    

    def traverse_stmt_ImportFrom_module(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_stmt_ImportFrom_names(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_ImportFrom_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- ImportWildCard
    def analyze_token_stmt_ImportWildCard(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_str(self.traverse_stmt_ImportWildCard_module(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_ImportWildCard_attributes(inher, synth_children)
    

    def traverse_stmt_ImportWildCard_module(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_ImportWildCard_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Global
    def analyze_token_stmt_Global(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_sequence_name(self.traverse_stmt_Global_names(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Global_attributes(inher, synth_children)
    

    def traverse_stmt_Global_names(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Global_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Nonlocal
    def analyze_token_stmt_Nonlocal(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_sequence_name(self.traverse_stmt_Nonlocal_names(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Nonlocal_attributes(inher, synth_children)
    

    def traverse_stmt_Nonlocal_names(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Nonlocal_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Expr
    def analyze_token_stmt_Expr(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_stmt_Expr_content(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_stmt_Expr_attributes(inher, synth_children)
    

    def traverse_stmt_Expr_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_stmt_Expr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Pass
    def analyze_token_stmt_Pass(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_stmt_Pass_attributes(inher, synth_children)
    

    def synthesize_stmt_Pass_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Break
    def analyze_token_stmt_Break(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_stmt_Break_attributes(inher, synth_children)
    

    def synthesize_stmt_Break_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # stmt <-- Continue
    def analyze_token_stmt_Continue(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_stmt_Continue_attributes(inher, synth_children)
    

    def synthesize_stmt_Continue_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # expr
    def analyze_token_expr(self, 
        token : Grammar, 
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:
        assert token.options == "expr"
        rule_name = token.selection

        if False: 
            pass
        
        elif rule_name == "BoolOp": 
            return self.analyze_token_expr_BoolOp(inher, children, stack_result, stack)
            

        elif rule_name == "AssignExpr": 
            return self.analyze_token_expr_AssignExpr(inher, children, stack_result, stack)
            

        elif rule_name == "BinOp": 
            return self.analyze_token_expr_BinOp(inher, children, stack_result, stack)
            

        elif rule_name == "UnaryOp": 
            return self.analyze_token_expr_UnaryOp(inher, children, stack_result, stack)
            

        elif rule_name == "Lambda": 
            return self.analyze_token_expr_Lambda(inher, children, stack_result, stack)
            

        elif rule_name == "IfExp": 
            return self.analyze_token_expr_IfExp(inher, children, stack_result, stack)
            

        elif rule_name == "Dictionary": 
            return self.analyze_token_expr_Dictionary(inher, children, stack_result, stack)
            

        elif rule_name == "EmptyDictionary": 
            return self.analyze_token_expr_EmptyDictionary(inher, children, stack_result, stack)
            

        elif rule_name == "Set": 
            return self.analyze_token_expr_Set(inher, children, stack_result, stack)
            

        elif rule_name == "ListComp": 
            return self.analyze_token_expr_ListComp(inher, children, stack_result, stack)
            

        elif rule_name == "SetComp": 
            return self.analyze_token_expr_SetComp(inher, children, stack_result, stack)
            

        elif rule_name == "DictionaryComp": 
            return self.analyze_token_expr_DictionaryComp(inher, children, stack_result, stack)
            

        elif rule_name == "GeneratorExp": 
            return self.analyze_token_expr_GeneratorExp(inher, children, stack_result, stack)
            

        elif rule_name == "Await": 
            return self.analyze_token_expr_Await(inher, children, stack_result, stack)
            

        elif rule_name == "YieldNothing": 
            return self.analyze_token_expr_YieldNothing(inher, children, stack_result, stack)
            

        elif rule_name == "Yield": 
            return self.analyze_token_expr_Yield(inher, children, stack_result, stack)
            

        elif rule_name == "YieldFrom": 
            return self.analyze_token_expr_YieldFrom(inher, children, stack_result, stack)
            

        elif rule_name == "Compare": 
            return self.analyze_token_expr_Compare(inher, children, stack_result, stack)
            

        elif rule_name == "Call": 
            return self.analyze_token_expr_Call(inher, children, stack_result, stack)
            

        elif rule_name == "CallArgs": 
            return self.analyze_token_expr_CallArgs(inher, children, stack_result, stack)
            

        elif rule_name == "Integer": 
            return self.analyze_token_expr_Integer(inher, children, stack_result, stack)
            

        elif rule_name == "Float": 
            return self.analyze_token_expr_Float(inher, children, stack_result, stack)
            

        elif rule_name == "ConcatString": 
            return self.analyze_token_expr_ConcatString(inher, children, stack_result, stack)
            

        elif rule_name == "True_": 
            return self.analyze_token_expr_True_(inher, children, stack_result, stack)
            

        elif rule_name == "False_": 
            return self.analyze_token_expr_False_(inher, children, stack_result, stack)
            

        elif rule_name == "None_": 
            return self.analyze_token_expr_None_(inher, children, stack_result, stack)
            

        elif rule_name == "Ellip": 
            return self.analyze_token_expr_Ellip(inher, children, stack_result, stack)
            

        elif rule_name == "Attribute": 
            return self.analyze_token_expr_Attribute(inher, children, stack_result, stack)
            

        elif rule_name == "Subscript": 
            return self.analyze_token_expr_Subscript(inher, children, stack_result, stack)
            

        elif rule_name == "Starred": 
            return self.analyze_token_expr_Starred(inher, children, stack_result, stack)
            

        elif rule_name == "Name": 
            return self.analyze_token_expr_Name(inher, children, stack_result, stack)
            

        elif rule_name == "List": 
            return self.analyze_token_expr_List(inher, children, stack_result, stack)
            

        elif rule_name == "EmptyList": 
            return self.analyze_token_expr_EmptyList(inher, children, stack_result, stack)
            

        elif rule_name == "Tuple": 
            return self.analyze_token_expr_Tuple(inher, children, stack_result, stack)
            

        elif rule_name == "EmptyTuple": 
            return self.analyze_token_expr_EmptyTuple(inher, children, stack_result, stack)
            

        elif rule_name == "Slice": 
            return self.analyze_token_expr_Slice(inher, children, stack_result, stack)
            
        else:
            raise AnalysisError()



    def analyze_expr(self, inher : I) -> S:

        stack : list[tuple[abstract_token, I, tuple[S, ...]]] = [(self.next(inher), inher, ())]

        stack_result : Optional[S] = None 
        while stack:

            (token, inher, children) = stack.pop()

            assert isinstance(token, Grammar)
            stack_result = self.analyze_token_expr(token, inher, children, stack_result, stack)

        assert stack_result
        return stack_result

    
    # expr <-- BoolOp
    def analyze_token_expr_BoolOp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 3

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_BoolOp_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_boolop(self.traverse_expr_BoolOp_op(inher, children))
            stack.append((Grammar("expr", "BoolOp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "BoolOp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_BoolOp_left(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 2 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "BoolOp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_BoolOp_right(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_BoolOp_left(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_BoolOp_op(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_BoolOp_right(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_BoolOp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- AssignExpr
    def analyze_token_expr_AssignExpr(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_AssignExpr_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "AssignExpr"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_AssignExpr_target(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "AssignExpr"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_AssignExpr_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_AssignExpr_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_AssignExpr_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_AssignExpr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- BinOp
    def analyze_token_expr_BinOp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 3

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_BinOp_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_operator(self.traverse_expr_BinOp_op(inher, children))
            stack.append((Grammar("expr", "BinOp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "BinOp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_BinOp_left(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 2 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "BinOp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_BinOp_right(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_BinOp_left(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_BinOp_op(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_BinOp_right(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_BinOp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- UnaryOp
    def analyze_token_expr_UnaryOp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_UnaryOp_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_unaryop(self.traverse_expr_UnaryOp_op(inher, children))
            stack.append((Grammar("expr", "UnaryOp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "UnaryOp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_UnaryOp_right(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_UnaryOp_op(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_UnaryOp_right(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_UnaryOp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Lambda
    def analyze_token_expr_Lambda(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Lambda_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_parameters(self.traverse_expr_Lambda_params(inher, children))
            stack.append((Grammar("expr", "Lambda"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Lambda"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Lambda_body(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Lambda_params(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_Lambda_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Lambda_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- IfExp
    def analyze_token_expr_IfExp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 3

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_IfExp_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "IfExp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_IfExp_body(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "IfExp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_IfExp_test(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 2 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "IfExp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_IfExp_orelse(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_IfExp_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_IfExp_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_IfExp_orelse(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_IfExp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Dictionary
    def analyze_token_expr_Dictionary(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Dictionary_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_dictionary_content(self.traverse_expr_Dictionary_content(inher, children))
            stack.append((Grammar("expr", "Dictionary"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Dictionary_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Dictionary_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- EmptyDictionary
    def analyze_token_expr_EmptyDictionary(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_EmptyDictionary_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_EmptyDictionary_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Set
    def analyze_token_expr_Set(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Set_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_comma_exprs(self.traverse_expr_Set_content(inher, children))
            stack.append((Grammar("expr", "Set"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Set_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Set_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- ListComp
    def analyze_token_expr_ListComp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_ListComp_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_comprehension_constraints(self.traverse_expr_ListComp_constraints(inher, children))
            stack.append((Grammar("expr", "ListComp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "ListComp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_ListComp_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_ListComp_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_ListComp_constraints(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_ListComp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- SetComp
    def analyze_token_expr_SetComp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_SetComp_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_comprehension_constraints(self.traverse_expr_SetComp_constraints(inher, children))
            stack.append((Grammar("expr", "SetComp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "SetComp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_SetComp_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_SetComp_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_SetComp_constraints(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_SetComp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- DictionaryComp
    def analyze_token_expr_DictionaryComp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 3

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_DictionaryComp_attributes(inher, children)
        
        elif index == 2: # index does *not* refer to an inductive child

            child_synth = self.analyze_comprehension_constraints(self.traverse_expr_DictionaryComp_constraints(inher, children))
            stack.append((Grammar("expr", "DictionaryComp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "DictionaryComp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_DictionaryComp_key(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "DictionaryComp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_DictionaryComp_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_DictionaryComp_key(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_DictionaryComp_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_DictionaryComp_constraints(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_DictionaryComp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- GeneratorExp
    def analyze_token_expr_GeneratorExp(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_GeneratorExp_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_comprehension_constraints(self.traverse_expr_GeneratorExp_constraints(inher, children))
            stack.append((Grammar("expr", "GeneratorExp"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "GeneratorExp"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_GeneratorExp_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_GeneratorExp_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_GeneratorExp_constraints(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_GeneratorExp_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Await
    def analyze_token_expr_Await(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Await_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Await"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Await_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Await_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Await_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- YieldNothing
    def analyze_token_expr_YieldNothing(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_YieldNothing_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_YieldNothing_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Yield
    def analyze_token_expr_Yield(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Yield_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Yield"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Yield_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Yield_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Yield_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- YieldFrom
    def analyze_token_expr_YieldFrom(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_YieldFrom_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "YieldFrom"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_YieldFrom_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_YieldFrom_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_YieldFrom_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Compare
    def analyze_token_expr_Compare(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Compare_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_comparisons(self.traverse_expr_Compare_comps(inher, children))
            stack.append((Grammar("expr", "Compare"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Compare"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Compare_left(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Compare_left(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_Compare_comps(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Compare_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Call
    def analyze_token_expr_Call(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Call_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Call"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Call_func(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Call_func(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Call_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- CallArgs
    def analyze_token_expr_CallArgs(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_CallArgs_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_arguments(self.traverse_expr_CallArgs_args(inher, children))
            stack.append((Grammar("expr", "CallArgs"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "CallArgs"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_CallArgs_func(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_CallArgs_func(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_CallArgs_args(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_CallArgs_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Integer
    def analyze_token_expr_Integer(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Integer_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_expr_Integer_content(inher, children))
            stack.append((Grammar("expr", "Integer"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Integer_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Integer_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Float
    def analyze_token_expr_Float(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Float_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_expr_Float_content(inher, children))
            stack.append((Grammar("expr", "Float"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Float_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Float_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- ConcatString
    def analyze_token_expr_ConcatString(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_ConcatString_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_sequence_string(self.traverse_expr_ConcatString_content(inher, children))
            stack.append((Grammar("expr", "ConcatString"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_ConcatString_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_ConcatString_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- True_
    def analyze_token_expr_True_(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_True__attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_True__attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- False_
    def analyze_token_expr_False_(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_False__attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_False__attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- None_
    def analyze_token_expr_None_(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_None__attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_None__attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Ellip
    def analyze_token_expr_Ellip(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Ellip_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_Ellip_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Attribute
    def analyze_token_expr_Attribute(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Attribute_attributes(inher, children)
        
        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_expr_Attribute_name(inher, children))
            stack.append((Grammar("expr", "Attribute"), inher, children + tuple([child_synth])))
            return None
            
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Attribute"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Attribute_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Attribute_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_Attribute_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Attribute_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Subscript
    def analyze_token_expr_Subscript(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 2

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Subscript_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Subscript"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Subscript_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            

        elif index == 1 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Subscript"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Subscript_slice(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Subscript_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_Subscript_slice(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Subscript_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Starred
    def analyze_token_expr_Starred(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        
        if stack_result:
            # get the result from the child in the stack
            children = children + tuple([stack_result]) 
        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Starred_attributes(inher, children)
        
        
        elif index == 0 : # index refers to an inductive child
            # put back current node
            stack.append((Grammar("expr", "Starred"), inher, children))

            # add on child node 
            child_inher = self.traverse_expr_Starred_content(inher, children)
            stack.append((self.next(child_inher), child_inher, ()))
            
        else:
            raise AnalysisError()
    
    def traverse_expr_Starred_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Starred_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Name
    def analyze_token_expr_Name(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Name_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_str(self.traverse_expr_Name_content(inher, children))
            stack.append((Grammar("expr", "Name"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Name_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Name_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- List
    def analyze_token_expr_List(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_List_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_comma_exprs(self.traverse_expr_List_content(inher, children))
            stack.append((Grammar("expr", "List"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_List_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_List_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- EmptyList
    def analyze_token_expr_EmptyList(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_EmptyList_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_EmptyList_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Tuple
    def analyze_token_expr_Tuple(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 1

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Tuple_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_comma_exprs(self.traverse_expr_Tuple_content(inher, children))
            stack.append((Grammar("expr", "Tuple"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Tuple_content(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Tuple_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- EmptyTuple
    def analyze_token_expr_EmptyTuple(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 0

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_EmptyTuple_attributes(inher, children)
        
        
        else:
            raise AnalysisError()
    
    def synthesize_expr_EmptyTuple_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # expr <-- Slice
    def analyze_token_expr_Slice(self,
        inher : I, children : tuple[S, ...], stack_result : Optional[S], 
        stack : list[tuple[abstract_token, I, tuple[S, ...]]]
    ) -> Optional[S]:

        

        total_num_children = 3

        index = len(children)
        if index == total_num_children:
            # the processing of the current rule has completed
            # return the analysis result to the previous item in the stack
            return self.synthesize_expr_Slice_attributes(inher, children)
        
        elif index == 0: # index does *not* refer to an inductive child

            child_synth = self.analyze_option_expr(self.traverse_expr_Slice_lower(inher, children))
            stack.append((Grammar("expr", "Slice"), inher, children + tuple([child_synth])))
            return None
            

        elif index == 1: # index does *not* refer to an inductive child

            child_synth = self.analyze_option_expr(self.traverse_expr_Slice_upper(inher, children))
            stack.append((Grammar("expr", "Slice"), inher, children + tuple([child_synth])))
            return None
            

        elif index == 2: # index does *not* refer to an inductive child

            child_synth = self.analyze_option_expr(self.traverse_expr_Slice_step(inher, children))
            stack.append((Grammar("expr", "Slice"), inher, children + tuple([child_synth])))
            return None
            
        
        else:
            raise AnalysisError()
    
    def traverse_expr_Slice_lower(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_Slice_upper(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_expr_Slice_step(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_expr_Slice_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # boolop
    def analyze_token_boolop(self, token : Grammar, inher : I) -> S:
        assert token.options == "boolop"

        if False:
            pass
        
        elif token.selection == "And":
            return self.analyze_token_boolop_And(inher)
            

        elif token.selection == "Or":
            return self.analyze_token_boolop_Or(inher)
            
        else:
            raise AnalysisError()

    def analyze_boolop(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_boolop(token, inher)

    
    # boolop <-- And
    def analyze_token_boolop_And(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_boolop_And_attributes(inher, synth_children)
    

    def synthesize_boolop_And_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # boolop <-- Or
    def analyze_token_boolop_Or(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_boolop_Or_attributes(inher, synth_children)
    

    def synthesize_boolop_Or_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # operator
    def analyze_token_operator(self, token : Grammar, inher : I) -> S:
        assert token.options == "operator"

        if False:
            pass
        
        elif token.selection == "Add":
            return self.analyze_token_operator_Add(inher)
            

        elif token.selection == "Sub":
            return self.analyze_token_operator_Sub(inher)
            

        elif token.selection == "Mult":
            return self.analyze_token_operator_Mult(inher)
            

        elif token.selection == "MatMult":
            return self.analyze_token_operator_MatMult(inher)
            

        elif token.selection == "Div":
            return self.analyze_token_operator_Div(inher)
            

        elif token.selection == "Mod":
            return self.analyze_token_operator_Mod(inher)
            

        elif token.selection == "Pow":
            return self.analyze_token_operator_Pow(inher)
            

        elif token.selection == "LShift":
            return self.analyze_token_operator_LShift(inher)
            

        elif token.selection == "RShift":
            return self.analyze_token_operator_RShift(inher)
            

        elif token.selection == "BitOr":
            return self.analyze_token_operator_BitOr(inher)
            

        elif token.selection == "BitXor":
            return self.analyze_token_operator_BitXor(inher)
            

        elif token.selection == "BitAnd":
            return self.analyze_token_operator_BitAnd(inher)
            

        elif token.selection == "FloorDiv":
            return self.analyze_token_operator_FloorDiv(inher)
            
        else:
            raise AnalysisError()

    def analyze_operator(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_operator(token, inher)

    
    # operator <-- Add
    def analyze_token_operator_Add(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_Add_attributes(inher, synth_children)
    

    def synthesize_operator_Add_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- Sub
    def analyze_token_operator_Sub(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_Sub_attributes(inher, synth_children)
    

    def synthesize_operator_Sub_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- Mult
    def analyze_token_operator_Mult(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_Mult_attributes(inher, synth_children)
    

    def synthesize_operator_Mult_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- MatMult
    def analyze_token_operator_MatMult(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_MatMult_attributes(inher, synth_children)
    

    def synthesize_operator_MatMult_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- Div
    def analyze_token_operator_Div(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_Div_attributes(inher, synth_children)
    

    def synthesize_operator_Div_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- Mod
    def analyze_token_operator_Mod(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_Mod_attributes(inher, synth_children)
    

    def synthesize_operator_Mod_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- Pow
    def analyze_token_operator_Pow(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_Pow_attributes(inher, synth_children)
    

    def synthesize_operator_Pow_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- LShift
    def analyze_token_operator_LShift(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_LShift_attributes(inher, synth_children)
    

    def synthesize_operator_LShift_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- RShift
    def analyze_token_operator_RShift(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_RShift_attributes(inher, synth_children)
    

    def synthesize_operator_RShift_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- BitOr
    def analyze_token_operator_BitOr(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_BitOr_attributes(inher, synth_children)
    

    def synthesize_operator_BitOr_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- BitXor
    def analyze_token_operator_BitXor(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_BitXor_attributes(inher, synth_children)
    

    def synthesize_operator_BitXor_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- BitAnd
    def analyze_token_operator_BitAnd(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_BitAnd_attributes(inher, synth_children)
    

    def synthesize_operator_BitAnd_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # operator <-- FloorDiv
    def analyze_token_operator_FloorDiv(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_operator_FloorDiv_attributes(inher, synth_children)
    

    def synthesize_operator_FloorDiv_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # unaryop
    def analyze_token_unaryop(self, token : Grammar, inher : I) -> S:
        assert token.options == "unaryop"

        if False:
            pass
        
        elif token.selection == "Invert":
            return self.analyze_token_unaryop_Invert(inher)
            

        elif token.selection == "Not":
            return self.analyze_token_unaryop_Not(inher)
            

        elif token.selection == "UAdd":
            return self.analyze_token_unaryop_UAdd(inher)
            

        elif token.selection == "USub":
            return self.analyze_token_unaryop_USub(inher)
            
        else:
            raise AnalysisError()

    def analyze_unaryop(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_unaryop(token, inher)

    
    # unaryop <-- Invert
    def analyze_token_unaryop_Invert(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_unaryop_Invert_attributes(inher, synth_children)
    

    def synthesize_unaryop_Invert_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # unaryop <-- Not
    def analyze_token_unaryop_Not(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_unaryop_Not_attributes(inher, synth_children)
    

    def synthesize_unaryop_Not_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # unaryop <-- UAdd
    def analyze_token_unaryop_UAdd(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_unaryop_UAdd_attributes(inher, synth_children)
    

    def synthesize_unaryop_UAdd_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # unaryop <-- USub
    def analyze_token_unaryop_USub(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_unaryop_USub_attributes(inher, synth_children)
    

    def synthesize_unaryop_USub_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # cmpop
    def analyze_token_cmpop(self, token : Grammar, inher : I) -> S:
        assert token.options == "cmpop"

        if False:
            pass
        
        elif token.selection == "Eq":
            return self.analyze_token_cmpop_Eq(inher)
            

        elif token.selection == "NotEq":
            return self.analyze_token_cmpop_NotEq(inher)
            

        elif token.selection == "Lt":
            return self.analyze_token_cmpop_Lt(inher)
            

        elif token.selection == "LtE":
            return self.analyze_token_cmpop_LtE(inher)
            

        elif token.selection == "Gt":
            return self.analyze_token_cmpop_Gt(inher)
            

        elif token.selection == "GtE":
            return self.analyze_token_cmpop_GtE(inher)
            

        elif token.selection == "Is":
            return self.analyze_token_cmpop_Is(inher)
            

        elif token.selection == "IsNot":
            return self.analyze_token_cmpop_IsNot(inher)
            

        elif token.selection == "In":
            return self.analyze_token_cmpop_In(inher)
            

        elif token.selection == "NotIn":
            return self.analyze_token_cmpop_NotIn(inher)
            
        else:
            raise AnalysisError()

    def analyze_cmpop(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_cmpop(token, inher)

    
    # cmpop <-- Eq
    def analyze_token_cmpop_Eq(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_Eq_attributes(inher, synth_children)
    

    def synthesize_cmpop_Eq_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- NotEq
    def analyze_token_cmpop_NotEq(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_NotEq_attributes(inher, synth_children)
    

    def synthesize_cmpop_NotEq_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- Lt
    def analyze_token_cmpop_Lt(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_Lt_attributes(inher, synth_children)
    

    def synthesize_cmpop_Lt_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- LtE
    def analyze_token_cmpop_LtE(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_LtE_attributes(inher, synth_children)
    

    def synthesize_cmpop_LtE_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- Gt
    def analyze_token_cmpop_Gt(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_Gt_attributes(inher, synth_children)
    

    def synthesize_cmpop_Gt_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- GtE
    def analyze_token_cmpop_GtE(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_GtE_attributes(inher, synth_children)
    

    def synthesize_cmpop_GtE_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- Is
    def analyze_token_cmpop_Is(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_Is_attributes(inher, synth_children)
    

    def synthesize_cmpop_Is_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- IsNot
    def analyze_token_cmpop_IsNot(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_IsNot_attributes(inher, synth_children)
    

    def synthesize_cmpop_IsNot_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- In
    def analyze_token_cmpop_In(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_In_attributes(inher, synth_children)
    

    def synthesize_cmpop_In_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # cmpop <-- NotIn
    def analyze_token_cmpop_NotIn(self, inher : I) -> S:
        synth_children = () 

        

        return self.synthesize_cmpop_NotIn_attributes(inher, synth_children)
    

    def synthesize_cmpop_NotIn_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # constraint
    def analyze_token_constraint(self, token : Grammar, inher : I) -> S:
        assert token.options == "constraint"

        if False:
            pass
        
        elif token.selection == "AsyncConstraint":
            return self.analyze_token_constraint_AsyncConstraint(inher)
            

        elif token.selection == "Constraint":
            return self.analyze_token_constraint_Constraint(inher)
            
        else:
            raise AnalysisError()

    def analyze_constraint(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        return self.analyze_token_constraint(token, inher)

    
    # constraint <-- AsyncConstraint
    def analyze_token_constraint_AsyncConstraint(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_constraint_AsyncConstraint_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_constraint_AsyncConstraint_search_space(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_constraint_filters(self.traverse_constraint_AsyncConstraint_filts(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_constraint_AsyncConstraint_attributes(inher, synth_children)
    

    def traverse_constraint_AsyncConstraint_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_constraint_AsyncConstraint_search_space(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_constraint_AsyncConstraint_filts(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_constraint_AsyncConstraint_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    
    # constraint <-- Constraint
    def analyze_token_constraint_Constraint(self, inher : I) -> S:
        synth_children = () 

        
        synth = self.analyze_expr(self.traverse_constraint_Constraint_target(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_expr(self.traverse_constraint_Constraint_search_space(inher, synth_children))
        synth_children += tuple([synth])
        
        synth = self.analyze_constraint_filters(self.traverse_constraint_Constraint_filts(inher, synth_children))
        synth_children += tuple([synth])
        

        return self.synthesize_constraint_Constraint_attributes(inher, synth_children)
    

    def traverse_constraint_Constraint_target(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_constraint_Constraint_search_space(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_constraint_Constraint_filts(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_constraint_Constraint_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
     

    
    # CompareRight
    def analyze_CompareRight(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "CompareRight"

        return self.analyze_token_CompareRight(token, inher)
    
    def analyze_token_CompareRight(self, token : Grammar, inher : I) -> S:
        assert token.selection == "CompareRight"

        synth_children = () 
        
        synth = self.analyze_cmpop(self.traverse_CompareRight_op(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_expr(self.traverse_CompareRight_rand(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_CompareRight_attributes(inher, synth_children)
    
    def traverse_CompareRight_op(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_CompareRight_rand(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_CompareRight_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # ExceptHandler
    def analyze_ExceptHandler(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "ExceptHandler"

        return self.analyze_token_ExceptHandler(token, inher)
    
    def analyze_token_ExceptHandler(self, token : Grammar, inher : I) -> S:
        assert token.selection == "ExceptHandler"

        synth_children = () 
        
        synth = self.analyze_except_arg(self.traverse_ExceptHandler_arg(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_statements(self.traverse_ExceptHandler_body(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_ExceptHandler_attributes(inher, synth_children)
    
    def traverse_ExceptHandler_arg(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_ExceptHandler_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_ExceptHandler_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # Param
    def analyze_Param(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "Param"

        return self.analyze_token_Param(token, inher)
    
    def analyze_token_Param(self, token : Grammar, inher : I) -> S:
        assert token.selection == "Param"

        synth_children = () 
        
        synth = self.analyze_str(self.traverse_Param_name(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_param_annotation(self.traverse_Param_anno(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_param_default(self.traverse_Param_default(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_Param_attributes(inher, synth_children)
    
    def traverse_Param_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_Param_anno(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_Param_default(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_Param_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # ClassDef
    def analyze_ClassDef(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "ClassDef"

        return self.analyze_token_ClassDef(token, inher)
    
    def analyze_token_ClassDef(self, token : Grammar, inher : I) -> S:
        assert token.selection == "ClassDef"

        synth_children = () 
        
        synth = self.analyze_str(self.traverse_ClassDef_name(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_bases(self.traverse_ClassDef_bs(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_statements(self.traverse_ClassDef_body(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_ClassDef_attributes(inher, synth_children)
    
    def traverse_ClassDef_name(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_ClassDef_bs(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_ClassDef_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_ClassDef_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # ElifBlock
    def analyze_ElifBlock(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "ElifBlock"

        return self.analyze_token_ElifBlock(token, inher)
    
    def analyze_token_ElifBlock(self, token : Grammar, inher : I) -> S:
        assert token.selection == "ElifBlock"

        synth_children = () 
        
        synth = self.analyze_expr(self.traverse_ElifBlock_test(inher, synth_children))
        synth_children += tuple([synth])
            
        synth = self.analyze_statements(self.traverse_ElifBlock_body(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_ElifBlock_attributes(inher, synth_children)
    
    def traverse_ElifBlock_test(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def traverse_ElifBlock_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_ElifBlock_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # ElseBlock
    def analyze_ElseBlock(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "ElseBlock"

        return self.analyze_token_ElseBlock(token, inher)
    
    def analyze_token_ElseBlock(self, token : Grammar, inher : I) -> S:
        assert token.selection == "ElseBlock"

        synth_children = () 
        
        synth = self.analyze_statements(self.traverse_ElseBlock_body(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_ElseBlock_attributes(inher, synth_children)
    
    def traverse_ElseBlock_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_ElseBlock_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
    

    # FinallyBlock
    def analyze_FinallyBlock(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Grammar)
        assert token.options == "FinallyBlock"

        return self.analyze_token_FinallyBlock(token, inher)
    
    def analyze_token_FinallyBlock(self, token : Grammar, inher : I) -> S:
        assert token.selection == "FinallyBlock"

        synth_children = () 
        
        synth = self.analyze_statements(self.traverse_FinallyBlock_body(inher, synth_children))
        synth_children += tuple([synth])
            
        return self.synthesize_FinallyBlock_attributes(inher, synth_children)
    
    def traverse_FinallyBlock_body(self, inher : I, synth_preds : tuple[S, ...]) -> I:
        return inher
    
    def synthesize_FinallyBlock_attributes(self, inher : I, synth_children : tuple[S, ...]) -> S:
        return self.default_synth(inher)
     


    def analyze_token_str(self, token : Vocab, inher : I) -> S:
        return self.default_synth(inher)

    def analyze_str(self, inher : I) -> S:
        token = self.next(inher)
        assert isinstance(token, Vocab)
        return self.analyze_token_str(token, inher)

    @abstractmethod
    def default_synth(self, inher : I) -> S: pass

    